{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Implementation in tensorflow\n",
    "\n",
    "* https://www.ece.ucsb.edu/Faculty/Rabiner/ece259/Reprints/tutorial%20on%20hmm%20and%20applications.pdf\n",
    "\n",
    "## Train using descent\n",
    "* DONE\n",
    "\n",
    "## Predict \n",
    "## Visualize model\n",
    "* TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward  Algorithm Tensorflow\n",
    "\n",
    "* single sequence\n",
    "* batch of sequences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "from tensorflow.python.util.tf_export import tf_export\n",
    "from tensorflow.python.data.experimental import group_by_window\n",
    "\n",
    "from tensorflow.python.framework import constant_op\n",
    "from tensorflow.python.framework import dtypes\n",
    "\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import check_ops\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate sequences and fit them in buckets\n",
    "# TODO use ragged tensors instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buckettrunc_by_sequence_length(\n",
    "                              element_length_func,\n",
    "                              bucket_boundaries,\n",
    "                              bucket_batch_sizes,\n",
    "                              drop_remainder=False):   \n",
    "    \n",
    "    # Map function\n",
    "    def element_to_bucket_id(*args):\n",
    "        \"\"\"Return int64 id of the length bucket for this element.\"\"\"\n",
    "           \n",
    "        \n",
    "        bucket_boundaries=[10, 15, 20]\n",
    "        seq_length = element_length_func(args)\n",
    "        \n",
    "        \n",
    "        err_msg = (\"Sequence length (%i) needs to be greater then the first bucket boundary (%i) .\"%(seq_length, bucket_boundaries[0]) )\n",
    "        tf.assert_greater(\n",
    "            tf.constant(seq_length, dtype=tf.dtypes.int64),\n",
    "            tf.constant(bucket_boundaries[0], dtype=tf.dtypes.int64),\n",
    "            message=err_msg)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        boundaries = sorted(list(bucket_boundaries)) # [10, 15, 20]\n",
    "        buckets_min = boundaries  # [10, 15, 20]\n",
    "        buckets_max = boundaries[1:] + [np.iinfo(np.int32).max]  # [15, 20, np.int.max]\n",
    "        conditions_c = math_ops.logical_and( # for each element, \n",
    "            math_ops.greater_equal(x=seq_length, y=buckets_min), # x >= y\n",
    "            math_ops.less(x=seq_length, y=buckets_max  ))  # x < y\n",
    "        bucket_id = math_ops.reduce_min(array_ops.where(conditions_c))\n",
    "        return bucket_id\n",
    "    \n",
    "    # Reduce function\n",
    "    def batching_fn(bucket_id, grouped_dataset):\n",
    "        batch_size = window_size_fn(bucket_id)  \n",
    "        boundaries = tf.constant(bucket_boundaries, dtype=tf.dtypes.int64)\n",
    "        bucket_boundary = boundaries[bucket_id]\n",
    "        begin = tf.constant(value=0, dtype=tf.dtypes.int64,name='seq_begin')          \n",
    "        \n",
    "        grouped_dataset = grouped_dataset.map(lambda seq: tf.slice(seq, begin=[begin], size=[bucket_boundary])) # truncate to bucket boundary\n",
    "        return grouped_dataset.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    \n",
    "    # Batch size functions\n",
    "    batch_sizes = tf.constant(bucket_batch_sizes, dtype=tf.dtypes.int64)\n",
    "    def window_size_fn(bucket_id):\n",
    "        window_size = batch_sizes[bucket_id]\n",
    "        return window_size\n",
    "    \n",
    "    def _apply_fn(dataset):\n",
    "        return dataset.apply(group_by_window(\n",
    "            key_func=element_to_bucket_id, \n",
    "            reduce_func=batching_fn, \n",
    "            window_size_func=window_size_fn)\n",
    "        )\n",
    "    \n",
    "    return _apply_fn\n",
    "\n",
    "def seq_len(seq):\n",
    "    return  tf.shape(seq)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method playground\n",
    "# TODO write test\n",
    "\n",
    "bucket_boundaries=[10, 15, 20]\n",
    "seq_length = 20\n",
    "\n",
    "err_msg = (\"Sequence length (%i) needs to be greater then the first bucket boundary (%i) .\"%(seq_length, bucket_boundaries[0]) )\n",
    "tf.assert_greater(\n",
    "    tf.constant(seq_length, dtype=tf.dtypes.int64),\n",
    "    tf.constant(bucket_boundaries[0], dtype=tf.dtypes.int64),\n",
    "    message=err_msg)\n",
    "\n",
    "boundaries = sorted(list(bucket_boundaries)) # [10, 15, 20]\n",
    "buckets_min = boundaries  # [10, 15, 20]\n",
    "buckets_max = boundaries[1:] + [np.iinfo(np.int32).max]  # [15, 20, np.int.max]\n",
    "conditions_c = tf.math.logical_and( # for each element, \n",
    "    tf.greater_equal(x=seq_length, y=buckets_min), # x >= y\n",
    "    tf.math.less(x=seq_length, y=buckets_max  ))  # x < y\n",
    "\n",
    "bucket_id = tf.math.reduce_min(tf.where(conditions_c))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0, 2, 0],\n",
      "       [0, 2, 2],\n",
      "       [1, 1, 0]], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "data = tf.data.TextLineDataset(\n",
    "    filenames=[\"data/hmm.dat\"], compression_type=None, buffer_size=None, num_parallel_reads=None\n",
    ")\n",
    "\n",
    "data = data.map(lambda line: tf.strings.split(line, sep=\",\") )\n",
    "data = data.map(lambda line: tf.strings.to_number(line, out_type=tf.dtypes.int32))\n",
    "\n",
    "data = data.map(lambda line: tf.slice(line, begin=[0], size=[3]  ))\n",
    "data = data.shuffle(buffer_size=3)\n",
    "\n",
    "data = data.batch(batch_size=3)\n",
    "\n",
    "print(list(data.take(1).as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = [0] * 2\n",
    "train_sequences[0] = np.array([\n",
    "    [0, 2, 0, 1, 2, 2,0,1,1,0,1,2],\n",
    "    [0, 1, 0, 1, 2, 2,0,0,2,0,1,2],\n",
    "    [0, 1, 0, 1, 2, 2,0,1,2,0,1,2],\n",
    "    [0, 2, 2, 0, 2, 1,0,1,0,0,1,2],\n",
    "    [1, 1, 0, 1, 2, 2,0,1,0,0,1,2],\n",
    "    [1, 1, 0, 1, 2, 1,0,0,2,0,1,2]\n",
    "])\n",
    "\n",
    "train_sequences[1] = np.array([\n",
    "    [0, 2, 0, 1, 2, 2,0,1,1],\n",
    "    [0, 1, 0, 1, 2, 2,0,0,2],\n",
    "    [0, 1, 0, 1, 2, 2,0,1,2],\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Model \n",
    "\n",
    "* TODO document\n",
    "* TODO test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, N=4, M=3, A=None, B=None, PI=None):\n",
    "        super(HMM, self).__init__()\n",
    "        \n",
    "        # TODO create \n",
    "        # A state_transistion_probabilities \n",
    "        # B emission_probabilities\n",
    "        # p \n",
    "        # Transition probabilities\n",
    "        if A is None:\n",
    "            init_A = tf.random.uniform(shape=(N,N), dtype=tf.dtypes.float32)\n",
    "            init_A = init_A / tf.reduce_sum(init_A, axis=0)\n",
    "            self.A = tf.Variable(name=\"state_transition_probabilities\", trainable=True, initial_value=init_A)\n",
    "        else:    \n",
    "            self.A = tf.Variable(name='state_transition_probabilities', trainable=True, initial_value=tf.constant(\n",
    "                A, dtype=tf.float32\n",
    "            ))\n",
    "                              \n",
    "        \n",
    "        # Emission probabilities\n",
    "        if B is None:\n",
    "            init_B = tf.random.uniform(shape=(M,N) , dtype=tf.dtypes.float32, seed=None, name=None)\n",
    "            init_B = init_B / tf.reduce_sum(init_B, axis=0)\n",
    "            self.B = tf.Variable(name='emission_probabilities', trainable=True, initial_value=init_B)          \n",
    "        else:   \n",
    "            self.B = tf.Variable(name='emission_probabilities', trainable=True, initial_value=tf.constant(\n",
    "               B, dtype=tf.float32\n",
    "            ))\n",
    "                                 \n",
    "        if PI is None:\n",
    "            # Initial state probabilities\n",
    "            self.pi = tf.Variable(name='pi', trainable=False, initial_value=tf.constant(\n",
    "                np.array([1/N]*N), dtype=tf.float32\n",
    "            ))\n",
    "        else:\n",
    "            self.pi = tf.Variable(name='pi', trainable=False, initial_value=tf.constant(\n",
    "                PI, dtype=tf.float32\n",
    "            ))\n",
    "            \n",
    "        # TODO sanity checks for input data \n",
    "        \n",
    "        self.normalize()\n",
    "\n",
    "\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        # inputs are of shape [batch_size, sequence_length]\n",
    "        \n",
    "        self.add_loss(self.log_likelihood(inputs))\n",
    "        return inputs  # this is the negative log-likelihood\n",
    "    \n",
    "    \n",
    "    def log_likelihood(self, obs_batch):\n",
    "        \n",
    "        #obs_batch = [batch_size, seq_len]\n",
    "        batch_size = tf.shape(obs_batch)[0]\n",
    "        seq_len = tf.shape(obs_batch)[1]     \n",
    "\n",
    "        def body(i, prev, sum_log_c):\n",
    "            # prev = [batch_size, N ]\n",
    "            #tf.print(i, prev, sum_log_c)\n",
    "            \n",
    "            o_slice = tf.gather(obs_batch, i, axis=1) # [batch_size]\n",
    "            #tf.print(o_slice)\n",
    "            emissions_prob = tf.gather(self.B, o_slice, axis=0) # [batch_size, N]\n",
    "            #tf.print(emissions_prob)\n",
    "            batch_A = tf.repeat(tf.expand_dims(tf.transpose(self.A),0), batch_size, axis=0) # [  batch_size, N, N ]\n",
    "            \n",
    "            prob_sum = tf.matmul(a=batch_A,b=tf.expand_dims(prev,2))  # [batch_size, N,N] * [batch_size, N, 1] => [ batch_size, N, 1]\n",
    "            prob_sum = tf.squeeze( prob_sum, 2) #  [ batch_size, N, 1] => [ batch_size, N ]\n",
    "            \n",
    "            f = emissions_prob * prob_sum # [ batch_size, N] => [ batch_size, N ]\n",
    "            \n",
    "            c = 1. / tf.reduce_sum(f, axis=1) # [ batch_size, N ] => [batch_size]\n",
    "            c = tf.expand_dims(c, 1) #  [batch_size] => [ batch_size, 1]\n",
    "            \n",
    "            next_loopvars =  tf.add(i, 1), f * c, tf.add(sum_log_c, tf.math.log(c))\n",
    "            \n",
    "            return next_loopvars\n",
    "\n",
    "        i = tf.constant(0) # counter for the while loop\n",
    "        condition = lambda i, prev, sum_log_c: tf.less(i, seq_len ) # abort condition for the while loo p\n",
    "        \n",
    "        pis =  tf.repeat(tf.expand_dims(self.pi,0), batch_size, axis=0)\n",
    "        log_likelihood_sum = tf.zeros(shape=(batch_size,1))\n",
    "        loopvars = [i,pis,log_likelihood_sum ] \n",
    "        \n",
    "        r = tf.while_loop(condition, body, loopvars)\n",
    "\n",
    "        return r[2] \n",
    "    \n",
    "    def normalize(self):\n",
    "        self.A.assign(self.A/tf.reduce_sum(self.A, axis=1))\n",
    "        self.B.assign(self.B/tf.reduce_sum(self.B, axis=0))\n",
    "        \n",
    "    def generate(self, steps=1):\n",
    "        pass \n",
    "                                             \n",
    "    def loss(self):\n",
    "        # forward log loss           \n",
    "        \n",
    "        return lambda y_true, y_predicted : y_predicted # because we predict the negative log likelihood already\n",
    "  \n",
    "# TODO load from dataset\n",
    "class NormalizeCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        \n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    ")\n",
    "\n",
    "hmm = HMM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "78.208626\n",
      "88.41903\n",
      "97.790955\n",
      "96.276794\n",
      "94.4719\n",
      "93.00278\n",
      "91.79845\n",
      "90.78998\n",
      "89.92912\n",
      "89.1826\n",
      "Start of epoch 1\n",
      "88.52693\n",
      "87.945206\n",
      "87.42476\n",
      "86.95595\n",
      "86.53122\n",
      "86.1445\n",
      "85.79089\n",
      "85.46636\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-22b9e0d50f39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_TileGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    819\u001b[0m   split_shape = array_ops.reshape(\n\u001b[1;32m    820\u001b[0m       array_ops.transpose(array_ops.stack([op.inputs[1], input_shape])), [-1])\n\u001b[0;32m--> 821\u001b[0;31m   \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m   \u001b[0;31m# Sum reduces grad along the first dimension for IndexedSlices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexedSlices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mrange\u001b[0;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[1;32m   1586\u001b[0m     \u001b[0;31m# This will revole the case where start/limit/delta's original's dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \u001b[0;31m# is different from provided dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1588\u001b[0;31m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m     \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m    771\u001b[0m   \u001b[0mbase_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m   if isinstance(x,\n\u001b[0;32m--> 773\u001b[0;31m                 (ops.Tensor, _resource_variable_type)) and base_type == x.dtype:\n\u001b[0m\u001b[1;32m    774\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cast\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_enum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_enum\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "steps = 100\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step in range(steps):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            hmm(train_sequences[step%2])\n",
    "            loss = tf.reduce_sum(hmm.losses) \n",
    "            \n",
    "        grads = tape.gradient(loss, hmm.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, hmm.trainable_weights))\n",
    "        hmm.normalize()\n",
    "        \n",
    "        if (step%10==0):\n",
    "            print(loss.numpy())\n",
    "      \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMM Implementation in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM():\n",
    "    \n",
    "       \n",
    "    def __init__(self, N=2, t=0, S=None, M=3, V=None, A=None, B=None, pi=None, train_stop_margin=0.000001):\n",
    "        #TODO sanity check with N, M and dimension of A, B\n",
    "        \n",
    "        self.N = N # number of hidden states in the HMM  (ergodic - each state can be reached from each other state); Number of urns in the example\n",
    "        self.t = t # the time step we are in\n",
    "        if S is None:\n",
    "            self.S = [\"S_%i\"%s for s in range(self.N)] # an array of states\n",
    "            self.N = N\n",
    "        else:\n",
    "            self.S = S\n",
    "            self.N = len(S)\n",
    "        self.q_t = None # the current state at time t\n",
    "        \n",
    "        if V is None:\n",
    "            self.M = M  # number of distinct observation symbols per state, i.e. the discrete alphabet size (colors in the example)\n",
    "            self.V = [\"v_%i\"%v for v in range(self.M)] # our alphabet of symbols that can be emitted            \n",
    "        else:\n",
    "            self.V = V\n",
    "            self.M = len(V)\n",
    "            \n",
    "        if A is None:\n",
    "            self.A = np.ones(shape=(self.N,self.N), dtype=np.float64)*(1/self.N) # our state transition probability matrix. an element a_ij contains the probability of transitioning to S_j in the next step, given that in timestep t I am in S_i, how likeli is it \n",
    "        else:\n",
    "            self.A = A\n",
    "            \n",
    "        if B is None:\n",
    "            self.B = np.ones(shape=(self.N,self.M), dtype=np.float64)*(1/self.M) # our symbol emission probability matrix. b_jk contains the probability of emitting symbol v_k when I am in state S_j at timestep t.\n",
    "        else:\n",
    "            self.B = B\n",
    "\n",
    "        if pi is None:\n",
    "            self.pi = [1/self.N]*self.N # initial state distribution. pi_i contains the probability of starting in state S_i. \n",
    "        else:\n",
    "            self.pi = pi\n",
    "            \n",
    "        self.train_stop_margin=train_stop_margin\n",
    "            \n",
    "        print(self.N,self.M)\n",
    "            \n",
    "    def generate(self, steps=1):\n",
    "        self.t = 1\n",
    "        state_trans_prob = self.pi # choose the start state from our starting distribution \n",
    "     \n",
    "        for t in range(1, 1+steps):\n",
    "            q_t = np.random.choice(self.N,1,p=state_trans_prob)[0]  # transition to next state\n",
    "            o_t = np.random.choice(self.M,1,p=self.B[q_t,:])[0] # choose the emission probability for the current state\n",
    "            state_trans_prob = self.A[q_t,:] # get next transition probabilities\n",
    "            print(\"Timestep: {}, Current State: {}, Emission: {}\".format(t, self.S[q_t], self.V[o_t]))\n",
    "            \n",
    "    def visualize(self):\n",
    "        pass # TODO generate graph and show as image \n",
    "    \n",
    "    def _sequence_valid(self, O):\n",
    "         # \n",
    "        if len(O)==0:\n",
    "            print(\"Sequence must have at least one observation, has length 0.\")\n",
    "            return False\n",
    "        \n",
    "        #sanity check\n",
    "        for o in O:\n",
    "            if o not in self.V:\n",
    "                print(\"symbol {} was not in the allowed set of symbol for this HMM: {}\".format(o, self.V))\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    \"\"\"\n",
    "        Returns P(O|h), i.e., the probability that the sequence of observations O was emitted by this HMM. \n",
    "    \"\"\"\n",
    "    def sequence_likelihood(self, O=[]):\n",
    "        if not self._sequence_valid(O): return False\n",
    "        T = len(O)\n",
    "        alpha = self.forward(O)\n",
    "        \n",
    "        return alpha[T-1,:].sum() # termination step (21)\n",
    "    \n",
    "    def forward(self, O=[]):\n",
    "        if not self._sequence_valid(O): return False\n",
    "        T = len(O)\n",
    "        \n",
    "        alpha=np.ndarray(shape=(T, self.N))\n",
    "        \n",
    "        # initialization step (19)\n",
    "        t=0\n",
    "        for i in range(self.N):\n",
    "            alpha[t,i] = self.pi[i]*self.B[i,self.V.index(o)]\n",
    "        \n",
    "        for t, in range(0, T-1): # induction step (20)\n",
    "            o = O[t+1]\n",
    "            for j in range(self.N):\n",
    "                state_trans_prob = sum([ alpha[t,i]*self.A[i,j] for i in range(self.N) ])       \n",
    "                alpha[t+1,j] =  state_trans_prob * self.B[j,self.V.index(o)]              \n",
    "                \n",
    "        return alpha\n",
    "        \n",
    "    \n",
    "    def backward(self, O=[]):\n",
    "        if not self._sequence_valid(O): return False\n",
    "        T = len(O)\n",
    "        beta = np.ndarray(shape(T, self.N))\n",
    "        \n",
    "        beta[T-1,:] = 1\n",
    "         \n",
    "        for t in range(T-2, -1, -1):             \n",
    "            for i in range(self.N):\n",
    "                beta[t,i] = sum( self.A[i,j]*self.B[j,self.V.index(O[t+1])]*beta[t+1,j] for j in range(self.N)  )\n",
    "\n",
    "        return beta\n",
    "    \n",
    "    def xi(self, O[]): # (36) the probability of being in state S_i at time t, and S_j at time t+1\n",
    "        if not self._sequence_valid(O): return False\n",
    "        T = len(O)\n",
    "        alpha = self.forward(O)\n",
    "        beta =  self.backward(O)\n",
    "        \n",
    "        xi = np.ndarray(T-1, self.N, self.N)\n",
    "        \n",
    "        for t, in range(0, T-1): # (37)\n",
    "            for i in range(self.N):\n",
    "                for j in range(self.N):\n",
    "                    xi[t, i, j] =  alpha[t, i] * self.A[i,j] *  self.B[j,self.V.index(O[t+1])] * beta[t+1, j]\n",
    "            \n",
    "            xi[t,:,:]=xi[t,:,:]/xi[t,:,:].sum()  \n",
    "            \n",
    "    def gamma(self, xi): # estimated  probability of being in a state\n",
    "        T_1 = xi.shape[0] # (38)               \n",
    "        gamma = np.ndarray(shape=(T_1, self.N))\n",
    "        for t in range(T_1):\n",
    "            for i in range(self.N):\n",
    "                gamma[t,i] = xi[t, i,:].sum()     \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        if you want to train multiple sequences, you need to add summation over R to all the terms \n",
    "    \"\"\"\n",
    "            \n",
    "    def train(self, O[]):\n",
    "        \n",
    "        if not self._sequence_valid(O): return False\n",
    "        # xi_ij = the probability of being in state S_i at the time t, and S_j at the time t+1(\n",
    "        T = len(O)\n",
    "        \n",
    "        loop=True\n",
    "        while loop:       \n",
    "            # 1) create new model h' = (A',B',pi') as a copy of h (A,B, pi)        \n",
    "            h_bar = deepcopy(self)\n",
    "\n",
    "            # 2) update parameters of h' \n",
    "            xi = h_bar.xi(O)\n",
    "            gamma = self.gamma(xi)\n",
    "\n",
    "            h_bar.pi = gamma[0,:].tolist() # (40a)\n",
    "            h_bar.A = xi.sum(axis=0) / gamma.sum(axis=0) # (40b) sum over t ()\n",
    "\n",
    "\n",
    "            for k in range(self.M): # (40c)\n",
    "                for j in range(self.N):\n",
    "                    h_bar.B[k,j] = gamma[np.where(np.array(O)==self.V[k]),j] / gamma.sum(axis=0, j) # sum over t       \n",
    "\n",
    "            # 3.1) if  h is similar enough h', abort \n",
    "\n",
    "            P_h_given_O = self.sequence_likelihood(O)\n",
    "            P_h_bar_given_O = h_bar.sequence_likelihood(O)\n",
    "            \n",
    "            updated_model_is_better=P_h_bar_given_O>P_h_given_O and P_h_bar_given_O-P_h_given_O>self.train_stop_margin\n",
    "\n",
    "            if (updated_model_is_better):\n",
    "                self.update_from(h_bar)\n",
    "            else:\n",
    "                return True\n",
    "           \n",
    "    def update_from(self, hmm):\n",
    "        self.A = hmm.A\n",
    "        self.B = hmm.B\n",
    "        self.pi = hmm.pi      \n",
    "    \n",
    "    def best_state_sequence(self, O=[]):\n",
    "        if not self._sequence_valid(O): return False\n",
    "        \n",
    "        for t, o in enumerate(O):\n",
    "            if t==0: # Initialization\n",
    "                delta = [self.pi[i]*self.B[i,self.V.index(o)] for i in range(self.N)] # (32a)                \n",
    "                psi = np.ndarray(shape=(len(O),self.N), dtype=np.int) \n",
    "                psi[t, :]=0 # (32b)\n",
    "            else: # Recursion\n",
    "                new_delta = [0]*self.N\n",
    "                for j in range(self.N):\n",
    "                    new_delta[j] = max( [delta[i]*self.A[i,j]  for i in range(self.N)] ) * self.B[j,self.V.index(o)] # (33a)\n",
    "                    psi[t, j] = np.argmax( [delta[i]*self.A[i,j]  for i in range(self.N)] ) # (33b)\n",
    "                delta = new_delta\n",
    "            print(delta)\n",
    "    \n",
    "        P_star = max(delta) # (34a)\n",
    "        q_star = np.argmax(delta) # (34b)\n",
    "        \n",
    "        path = [-1]*len(O)\n",
    "        path[-1]=q_star  # last element of our sequence is q_star\n",
    "        next_q = q_star\n",
    "        print(path)\n",
    "  \n",
    "        \n",
    "        for t in range( len(O)-2, -1, -1): # we use -1 becase we already have q_star in our list. all the other values need to be an extra -1 because of how python reverse counting works.\n",
    "            print(t)\n",
    "            next_q = psi[t+1, next_q] # (35)\n",
    "            path[t] = next_q\n",
    "                                    \n",
    "        return [self.S[p] for p in path]                     \n",
    "                     \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a sequence of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h = HMM()\n",
    "h.generate(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = [\"v_0\", \"v_1\", \"v_2\", \"v_3\", \"v_4\"]\n",
    "O = [\"v_2\",\"v_1\", \"v_4\", \"v_1\"]\n",
    "\n",
    "k = 1\n",
    "\n",
    "mask = np.zeros(len(O))\n",
    "mask[np.where(np.array(O)==V[k])]=1\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax([1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given a sequence of observation, how likely is it that our model emitted such a sequence\n",
    "* Problem 1 in the paper - calculate P(O|$\\lambda$), where $\\lambda$= (A, B, pi)\n",
    "* Allows us to choose the model which best matches an observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HMM()\n",
    "\n",
    "h.sequence_likelihood(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check -sum of all combinations has to be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "chain_length = 3  # any int > 0\n",
    "all_combos = list(product(*(h.V,) * len(O)))\n",
    "all_likelihoods = list(map(lambda o: h.sequence_likelihood(o), all_combos))\n",
    "print(\"All possible all_likelihoods added: {}.\".format(sum(all_likelihoods)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give a model and an observation seqeunce, what is the optimal state transition sequence \n",
    "* problem 2 in paper\n",
    "* given $O$, calculate the most likely $Q$\n",
    "* multiple correct answers\n",
    "* Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=np.array([\n",
    "    [0.05,0.05,0.9],# from S0, goto S2\n",
    "    [0.1,0.7,0.2],# from S1, goto S1\n",
    "    [0.6,0.1,0.3] # from S2, goto S0\n",
    "])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=np.array([\n",
    "    [0.05,0.05, 0.8, 0.05, 0.05], # from S0, emit v2\n",
    "    [0.1 ,0.5 , 0.2, 0.1 , 0.1 ], # from S1, emit v1\n",
    "    [0.1 ,0.1 , 0.1, 0.1 , 0.6 ]  # from S2, emit v4\n",
    "])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HMM(A=A, B=B, M=5, N=3)\n",
    "  \n",
    "        \n",
    "O = [\"v_2\",\"v_1\", \"v_4\", \"v_1\"]\n",
    "h.best_state_sequence(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning the Parameters of the HMM using baum welch algorithm\n",
    "* EM expectation modification method\n",
    "* iterative approach, stops at maximum likelihood. leads to local maxima only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
